{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "962df649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from general import table_to_numpy, TitledModel, plot_loss_history, custom_objects, HDF5File, MaxPassLayer, feat, load_normalization\n",
    "\n",
    "\n",
    "def load_data(path, inputs, targets):\n",
    "    with HDF5File(path, \"r\") as f:\n",
    "        data = f[\"data\"]\n",
    "        offset = f[\"offset\"]\n",
    "        scale = f[\"scale\"]\n",
    "    data[\"weight\"] = data[\"weight\"] * scale[\"weight\"] + offset[\"weight\"]\n",
    "\n",
    "    weight = abs(np.asarray(data[\"weight\"]))\n",
    "    data_x = [table_to_numpy(data, input) for input in inputs]\n",
    "    if isinstance(targets[0], list):\n",
    "        data_y = []\n",
    "        for target in targets:\n",
    "            data_y.append(table_to_numpy(data, target))\n",
    "    else:\n",
    "        data_y = table_to_numpy(data, targets)\n",
    "    return data_x, data_y, weight\n",
    "\n",
    "\n",
    "def make_model(input_shapes, input_titles, prev_models, num_layers_per_input, num_layers, num_nodes, activation, dropout,\n",
    "               output_titles, lr, decay):\n",
    "    if not isinstance(activation, (list, tuple)):\n",
    "        activation = [activation]\n",
    "    if not isinstance(num_nodes, (list, tuple)):\n",
    "        num_nodes = [num_nodes]\n",
    "\n",
    "    inputs = []\n",
    "    for input_shape in input_shapes:\n",
    "        inputs.append(keras.Input(shape=input_shape[1:]))\n",
    "    xs = []\n",
    "    input_offset = 0\n",
    "    for prev_model in prev_models:\n",
    "        xs.append(prev_model(inputs[input_offset:input_offset + len(prev_model.input_titles)]))\n",
    "        input_offset += len(prev_model.input_titles)\n",
    "    xs += inputs[input_offset:]\n",
    "    xs_next = []\n",
    "    for i, num_layer_input in enumerate(num_layers_per_input):\n",
    "        x = xs[i]\n",
    "        for j in range(num_layer_input):\n",
    "            act = activation[j % len(activation)]\n",
    "            nn = num_nodes[j % len(num_nodes)]\n",
    "            x = keras.layers.Dense(nn, activation=act)(x)\n",
    "            if dropout:\n",
    "                x = keras.layers.Dropout(dropout)(x)\n",
    "        if len(xs[i].shape) == 3:\n",
    "            x = keras.layers.Flatten()(x)\n",
    "        xs_next.append(x)\n",
    "    del xs\n",
    "    x = keras.layers.Concatenate()(xs_next)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        act = activation[i % len(activation)]\n",
    "        nn = num_nodes[i % len(num_nodes)]\n",
    "        x = keras.layers.Dense(nn, activation=act, name=f\"reg_tt{i}\")(x)\n",
    "        if dropout:\n",
    "            x = keras.layers.Dropout(dropout)(x)\n",
    "    reg_activation = \"linear\"\n",
    "    loss = \"mean_squared_error\"\n",
    "    if isinstance(output_titles[0], list):\n",
    "        reg = []\n",
    "        for output_title in output_titles:\n",
    "            pname = output_title[0].split(\"_\")[0]\n",
    "            reg.append(keras.layers.Dense(len(output_title), activation=reg_activation, name=pname)(x))\n",
    "    else:\n",
    "        reg = keras.layers.Dense(len(output_titles), activation=reg_activation, name=\"reg_tt\")(x)\n",
    "    model = TitledModel(input_titles, output_titles, inputs=inputs, outputs=reg)\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr, decay=decay),\n",
    "        metrics=[],\n",
    "        weighted_metrics=[])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ce01cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path       = '/nfs/dust/cms/user/stafford/For_Emanuele/reconn/skims/TTTo2L2Nu_TuneCP5_13TeV-powheg-pythia8/'\n",
    "train_file = path + 'traindata.hdf5'\n",
    "valid_file = path + 'validatedata.hdf5'\n",
    "modelb     = 'model_bcls.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79bea0d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer count mismatch when loading weights from file. Model expected 0 layers, found 5 saved layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1512033/2340029921.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mb_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPassLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model_bbar = TitledModel(\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_titles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_104/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/keras/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     )\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_104/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_104/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/keras/saving/legacy/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, model)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0mlayer_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_layer_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    809\u001b[0m             \u001b[0;34m\"Layer count mismatch when loading weights from file. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0;34mf\"Model expected {len(filtered_layers)} layers, found \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer count mismatch when loading weights from file. Model expected 0 layers, found 5 saved layers."
     ]
    }
   ],
   "source": [
    "\n",
    "model_b = keras.models.load_model(modelb, custom_objects=custom_objects)\n",
    "\n",
    "b_out = MaxPassLayer(2)([model_b.output, model_b.inputs[0]])\n",
    "model_bbar = TitledModel(\n",
    "    model_b.input_titles,\n",
    "    [p + t.split(\"_\")[1] for t in model_b.input_titles[0] for p in (\"bot_\", \"abot_\")],\n",
    "    inputs=model_b.inputs, outputs=[b_out])\n",
    "model_bbar.trainable = False\n",
    "\n",
    "inputs = model_b.input_titles + [[\"met_pt\", \"met_phi\", \"met_x\", \"met_y\"] + feat(\"lep\") + feat(\"alep\")]\n",
    "targets = [\n",
    "    [\"top_x\", \"top_y\", \"top_z\", \"top_mass\"],\n",
    "    [\"atop_x\", \"atop_y\", \"atop_z\", \"atop_mass\"],\n",
    "]\n",
    "train = load_data(train_file, inputs, targets)\n",
    "validate = load_data(valid_file, inputs, targets)\n",
    "model_tt = make_model([t.shape for t in train[0]], inputs, [model_bbar], [0, 2], 3, 800, \"relu\", 0.25, targets, 0.001, 0)\n",
    "norm = load_normalization(train_file)\n",
    "keras.models.save_model(model_tt, \"model_tt.hdf5\")\n",
    "earlystop = keras.callbacks.EarlyStopping(patience=6)\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    \"model_tt.hdf5\", save_best_only=True, verbose=1)\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 2**13\n",
    "history = model_tt.fit(\n",
    "    train[0], train[1], sample_weight=train[2], batch_size=batch_size,\n",
    "    epochs=epochs, validation_data=validate,\n",
    "    callbacks=[earlystop, checkpoint])\n",
    "plot_loss_history(history, \"loss_tt.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a002e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
